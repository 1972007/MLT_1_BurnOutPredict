# -*- coding: utf-8 -*-
"""ml_predictive_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QeLpRQCtV2pUlZYpZvKuCzKGO4K4Focp

Koneksi ke Google Drive
"""

#Karena datasetnya dari Kaggle, file kaggle.json diupload ke google drive untuk kenyamanan
from google.colab import drive
drive.mount('/content/drive')

"""Pemasangan Kaggle"""

#Install kaggle API agar bisa mengunduh dataset kaggle
!pip install kaggle

#Perintah linux untuk membuat folder ~/.kaggle/ dan mengcopy file kaggle.json ke folder tersebut
! mkdir ~/.kaggle/
! cp drive/MyDrive/kaggle.json ~/.kaggle/

#Pengunduhan dataset kaggle
!kaggle datasets download -d blurredmachine/are-your-employees-burning-out

"""Ekstrasi File"""

#Ekstrasi data. Untuk kali ini , file test tidak akan dipakai karena tidak adanya label target
import zipfile
local_zip = "/content/are-your-employees-burning-out.zip"
dest_zip = "/content/are-your-employees-burning-out"
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall(dest_zip)
zip_ref.close()

"""Import library yang dibutuhkan"""

# Commented out IPython magic to ensure Python compatibility.
# Import pustaka
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""Pemasukan file kedalam dataframe Pandas"""

#Pengecekan file. parse_dates digunakan agar data kolom "Date of Joining" lebih mudah diproses
employees_train = pd.read_csv("/content/are-your-employees-burning-out/train.csv",parse_dates=["Date of Joining"])

employees_train

"""Pengecekan tipe data dalam dataframe"""

print(employees_train.dtypes)
employees_train.describe()

"""Pengecekan tanggal paling awal dan paling akhir"""

#Melihat tanggal maksimal dan minimal
min_date = min(employees_train["Date of Joining"])
max_date = max(employees_train["Date of Joining"])
min_date, max_date, max_date-min_date

"""Pengecekan nilai missing value"""

employees_train.isnull().sum()

employees_train[employees_train.isnull().any(axis=1)]

"""Pembuangan data yang tidak lengkap"""

#Pembuangan data-data tidak lengkap
employees_train_nonNull = employees_train.dropna()
print(employees_train_nonNull.isnull().sum())
print(employees_train_nonNull.describe())

employees_train_nonNull

"""Penyimpanan kolom fitur"""

features = list(employees_train_nonNull.columns)
features.remove("Burn Rate")

"""Pengubahan data Date of Training menjadi Join Days



"""

employees_train_nonNull["Join Days"] = (max(employees_train_nonNull["Date of Joining"])-employees_train_nonNull["Date of Joining"]).dt.days
employees_train_nonNull.drop(columns=["Date of Joining"], axis=1, inplace=True)
cat_features=["Gender","Company Type","WFH Setup Available"]

"""Plotting data kategorikal"""

employees_train_nonNull[cat_features[0]].value_counts().plot(kind='bar', title=cat_features[0])

employees_train_nonNull[cat_features[1]].value_counts().plot(kind='bar', title=cat_features[1])

employees_train_nonNull[cat_features[2]].value_counts().plot(kind='bar', title=cat_features[2])

"""Plotting data numerikal"""

employees_train_nonNull.hist(bins=30, figsize=(30,25))
plt.show()

"""Plotting data terhadap burn rate"""

for column in employees_train_nonNull.select_dtypes(include='object').columns.to_list()[1:]:
  sns.catplot(x=column, y="Burn Rate",height=10, data=employees_train_nonNull)
  sns.catplot(x=column, y="Burn Rate",height=10,kind="bar", data=employees_train_nonNull)
  plt.title(column + " Terhadap Burn Rate")

sns.pairplot(employees_train_nonNull, diag_kind = 'kde')

"""Korelasi data numerik terhadap Burn Rate"""

correlation_matrix = employees_train_nonNull.corr().round(2)
 
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)

"""Pengubahan Kolom kategorikal menjadi numerik"""

#Mengubah kolom kolom tertentu menjadi angka sehingga lebih mudah diproses (datanya seragam)

target = ["Burn Rate"]
def feature_encode(pd_df):
 try:
    pd_df["IsMale"] = pd.get_dummies(pd_df["Gender"], drop_first=True)
    pd_df["IsServiceCompany"]  = pd.get_dummies(pd_df["Company Type"], drop_first=True)
    pd_df["WFH Setup Available"]  = pd.get_dummies(pd_df["WFH Setup Available"], drop_first=True)


    pd_df.drop(columns=["Gender", "Company Type"], axis=1, inplace=True)
 except:
    pass
feature_encode(employees_train_nonNull)
employees_train_nonNull

features, target

"""Pengecekan Kuantil 1 dan 3"""

#Pengecekan outliers Q3
print(employees_train_nonNull.quantile(0.25))
print(employees_train_nonNull.quantile(0.75))
Q1, Q3 = (employees_train_nonNull.quantile(0.25),employees_train_nonNull.quantile(0.75))
IQR= Q3-Q1
employees_train_nonNull[((employees_train_nonNull>(Q3+1.5*IQR))).all(axis=1)]

#Pengecekan outliers Q1
employees_train_nonNull[((employees_train_nonNull<(Q1-1.5*IQR))).all(axis=1)]

"""Splitting dataset menjadi train dan valid"""

#Splitting
from sklearn.model_selection import train_test_split
Y = employees_train_nonNull["Burn Rate"]
X = employees_train_nonNull.drop(["Burn Rate","Employee ID"], axis=1)
X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1)

"""Standarisasi data Training dan Valid agar mudah diolah algoritma Machine Learning"""

#Standarize
from sklearn.preprocessing import StandardScaler
num_features = ["Join Days", "Designation", "Mental Fatigue Score", "Resource Allocation"]
def standarize(x):
  scaler = StandardScaler()
  scaler.fit(x[num_features])
  x[num_features] = scaler.transform(x.loc[:, num_features])
standarize(X_train)
standarize(X_val)

X_train[num_features].describe().round(6)

"""Training and Validating"""

#Dataframe untuk kemudahan evaluasi
df_models = pd.DataFrame(columns=["train_mse","val_mse"],
                        index=["knn","randfor","boost"])

#Training KNN
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor
from sklearn.metrics import mean_squared_error

knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, Y_train)

df_models.loc['knn','train_mse'] = mean_squared_error(y_pred=knn.predict(X_train), y_true=Y_train)
df_models.loc['knn','val_mse'] = mean_squared_error(y_pred=knn.predict(X_val), y_true=Y_val)
df_models

#Training RF
rf = RandomForestRegressor(n_estimators=40, max_depth=16, random_state=3, n_jobs=-1)
rf.fit(X_train, Y_train)
df_models.loc['randfor','train_mse'] = mean_squared_error(y_pred=rf.predict(X_train), y_true=Y_train)
df_models.loc['randfor','val_mse'] = mean_squared_error(y_pred=rf.predict(X_val), y_true=Y_val)
df_models

#Training Boosting
boosting=AdaBoostRegressor(learning_rate=0.01, random_state=3)
boosting.fit(X_train, Y_train)
df_models.loc['boost','train_mse'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=Y_train)
df_models.loc['boost','val_mse'] = mean_squared_error(y_pred=boosting.predict(X_val), y_true=Y_val)
df_models

#Graf pengecekan. MSE terkecil diraih Random Forest Algorithm
fix,ax = plt.subplots()
df_models.sort_values(by='val_mse',axis=0, ascending=False).plot(kind="barh",ax=ax,zorder=3)
ax.grid(zorder=0)